{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0095f-35c4-474a-b354-9305e4e756c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87321cc0-dcf9-4a0a-bcd9-169a99531cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for all hyperparameters.\"\"\"\n",
    "    # Model parameters\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    lm_embedding_dim = 384      # MiniLM's output dimension\n",
    "    mlp_hidden_dim = 768        # Monotonic MLP hidden dimension\n",
    "    mlp_num_layers = 1          # Monotonic MLP number of hidden layers\n",
    "    max_seq_length = 128        # Max sequence length for tokenizer\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 40                 # Maximum number of epochs used for training\n",
    "    patience = 10               # Number of epochs after which early stopping is triggered\n",
    "    batch_size = 128            # Mini-batch size\n",
    "    lr_lm = 2e-7                # Learning rate for fine-tuning the language model encoder\n",
    "    lr_mlp = 2e-4               # Learning rate for the monotonic MLP head \n",
    "    temperature = 0.05          # Temperature parameter for SupCon loss. Controls the importance of hard negatives in the learning task\n",
    "    seed = 42                   # random number generator seed\n",
    "    is_ablation = False         # Set to True to bypass the monotonic MLP head for the ablation study training run.\n",
    "\n",
    "    # System parameters\n",
    "    base_results_dir = './results_SNLI' # Only the base path is needed here\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Config()                  # Create an instance of the Config class to pass arguments to functions as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9676548-12d2-4fd1-9fce-71e8859e473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SETUP REPRODUCIBILITY AND DIRECTORIES\n",
    "# =============================================================================\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "run_name = (\n",
    "    f\"temp_{args.temperature}_bs_{args.batch_size}_lr_encoder_{args.lr_lm}_epochs_{args.epochs}_patience_{args.patience}\"\n",
    "    f\"{f'_lr_head_{args.lr_mlp}' if not args.is_ablation else ''}\"  # Conditionally add head learning rate\n",
    "    f\"{'_ablation' if args.is_ablation else ''}\"\n",
    ")\n",
    "loading_dir = os.path.join(args.base_results_dir, run_name)\n",
    "os.makedirs(loading_dir, exist_ok=True)\n",
    "print(f\"Best model will be loaded from this directory: {loading_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e61f4-d62f-44c7-8694-66b65222bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "class MonotonicMLP(nn.Module):\n",
    "    \"\"\"Monotonic MLP implemented using positive weights and nondecreasing activation functions.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int):  \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            current_dim = hidden_dim\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(current_dim, input_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:    # Implement monotonicity constraints via squared weights and non-decreasing activations\n",
    "        z = x\n",
    "        for layer in self.hidden_layers:\n",
    "            positive_weight = layer.weight**2    # Using squared weights to ensure positivity in the hidden layers during the forward pass\n",
    "            z = F.leaky_relu(F.linear(z, positive_weight, layer.bias))  # Uses the non-decreasing Leaky Relu activation function\n",
    "        positive_weight_out = self.output_layer.weight**2\n",
    "        z = F.linear(z, positive_weight_out, self.output_layer.bias)    # Squared weights are used for positivity in the output as well\n",
    "        return z\n",
    "\n",
    "class MonoCon_LM(nn.Module):\n",
    "    \"\"\"Metric learning model using a pre-trained LM and a Monotonic MLP head.\"\"\"\n",
    "    def __init__(self, model_name: str, lm_embedding_dim: int, mlp_hidden_dim: int, mlp_num_layers: int, is_ablation: bool = False):\n",
    "        super().__init__()\n",
    "        self.is_ablation = is_ablation      # Flag indicating whether to run the full model or ablation study\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)     # Sentence transformer encoder (MiniLM) specified in the Config class\n",
    "        if not self.is_ablation:\n",
    "            self.monotonic_mlp = MonotonicMLP(lm_embedding_dim, mlp_hidden_dim, mlp_num_layers)  # Monotonic MLP is created only for the full model\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):    # Generates a single average vector from multiple contextual embedding vectors \n",
    "        token_embeddings = model_output[0]                    # Extracts token embeddings from the model output\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()   # Accounts for variable sentence lengths by using padded dimensions that don't contribute to averaging\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        y_encoder_tokens = self.encoder(**kwargs)     # The MiniLM encoder generates sequences of tokens with contextual embeddings for each token\n",
    "        y_encoder = self._mean_pooling(y_encoder_tokens, kwargs['attention_mask'])    # Averages the contextual embeddings to yield a single sentence embedding vector \n",
    "        y = y_encoder if self.is_ablation else self.monotonic_mlp(y_encoder)          # MLP is only used to get embeddings for the full model\n",
    "        return y\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch dataset that contains only those sentence pairs from the full SNLI dataset that have an entailment relationship. \n",
    "       These constitute positive pairs for the purposes of supervised contrastive learning\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.samples = []\n",
    "        for example in data:\n",
    "            if example['label'] == 0: # The label 0 indicates entailment\n",
    "                self.samples.append((example['premise'], example['hypothesis']))\n",
    "\n",
    "    def __len__(self):      # Returns the total number of entailment pairs, i.e. the total dataset size\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):    # Returns a sentence pair based on its index\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb09bf3-827f-43f2-92fe-f231c641c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCTIONS TO LOAD DATA AND COMPUTE EMBEDDINGS\n",
    "# =============================================================================\n",
    "def create_collate_fn(tokenizer, max_length):\n",
    "    def collate_fn(batch):\n",
    "        premises = [item[0] for item in batch]\n",
    "        hypotheses = [item[1] for item in batch]\n",
    "        sentences = premises + hypotheses\n",
    "        tokenized = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        labels = torch.arange(len(premises)).repeat(2)\n",
    "        return tokenized, labels\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def compute_embeddings_for_stsb(model, tokenizer, stsb_data, device, args):\n",
    "    model.eval()\n",
    "    sents1 = [ex['sentence1'] for ex in stsb_data]\n",
    "    sents2 = [ex['sentence2'] for ex in stsb_data]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_embeds(sentences):\n",
    "        all_embeds = []\n",
    "        pbar_embed = tqdm(range(0, len(sentences), args.batch_size), desc=\"Computing embeddings\", leave=False)\n",
    "        for start_idx in pbar_embed:\n",
    "            batch_sents = sentences[start_idx : start_idx + args.batch_size]\n",
    "            tokenized = tokenizer(batch_sents, padding=True, truncation=True, max_length=args.max_seq_length, return_tensors='pt')\n",
    "            tokenized = {k: v.to(device) for k, v in tokenized.items()}\n",
    "            y_batch = model(**tokenized)\n",
    "            all_embeds.append(y_batch.cpu())\n",
    "        return torch.cat(all_embeds, dim=0)\n",
    "\n",
    "    embeds1 = get_embeds(sents1)\n",
    "    embeds2 = get_embeds(sents2)\n",
    "    return embeds1, embeds2\n",
    "\n",
    "\n",
    "def evaluate_stsb(model, tokenizer, stsb_data, args, silent=False):\n",
    "    if not silent:\n",
    "        print(f\"\\n--- Evaluating on STSb Benchmark ---\")\n",
    "    \n",
    "    embeddings1, embeddings2 = compute_embeddings_for_stsb(model, tokenizer, stsb_data, args.device, args)\n",
    "    \n",
    "    embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n",
    "    embeddings2 = F.normalize(embeddings2, p=2, dim=1)\n",
    "    \n",
    "    cosine_scores = torch.sum(embeddings1 * embeddings2, dim=1)\n",
    "    labels = torch.tensor([ex['score'] for ex in stsb_data])\n",
    "    \n",
    "    spearman_corr, _ = spearmanr(cosine_scores.numpy(), labels.numpy())\n",
    "    \n",
    "    if not silent:\n",
    "        print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    return spearman_corr\n",
    "\n",
    "\n",
    "@torch.no_grad(   )\n",
    "def generate_embeddings(sentences, model, tokenizer, args):\n",
    "    \"\"\"Computes embeddings for a given list of sentences.\"\"\"\n",
    "    model.eval()\n",
    "    all_embeds = []\n",
    "    # Create a progress bar for embedding generation\n",
    "    pbar_embed = tqdm(\n",
    "        range(0, len(sentences), args.batch_size),\n",
    "        desc=f\"Computing embeddings for {len(sentences)} sentences\",\n",
    "        leave=False\n",
    "    )\n",
    "    for start_idx in pbar_embed:\n",
    "        batch_sents = sentences[start_idx : start_idx + args.batch_size]\n",
    "        tokenized = tokenizer(\n",
    "            batch_sents,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=args.max_seq_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokenized = {k: v.to(args.device) for k, v in tokenized.items()}\n",
    "        y_batch = model(**tokenized)\n",
    "        all_embeds.append(y_batch.cpu())\n",
    "    return torch.cat(all_embeds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb2f7d-045f-406e-b26a-db25f9d6265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASETS AND DATALOADERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Loading tokenizer and datasets ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "snli_dataset_raw = load_dataset('snli', split='train').filter(lambda ex: ex['label'] in [0, 1, 2])\n",
    "train_dataset = SNLIDataset(snli_dataset_raw)\n",
    "collate_fn = create_collate_fn(tokenizer, args.max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=0)\n",
    "\n",
    "stsb_val_data = list(load_dataset('sentence-transformers/stsb', split='validation'))\n",
    "stsb_test_data = list(load_dataset('sentence-transformers/stsb', split='test'))\n",
    "\n",
    "print(\"Data loaded successfully.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e824f-46a2-4398-b760-2a058bbf9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE BEST MODEL\n",
    "# =============================================================================\n",
    "\n",
    "model = MonoCon_LM(\n",
    "    model_name=args.model_name,\n",
    "    lm_embedding_dim=args.lm_embedding_dim,\n",
    "    mlp_hidden_dim=args.mlp_hidden_dim,\n",
    "    mlp_num_layers=args.mlp_num_layers,\n",
    "    is_ablation=args.is_ablation,\n",
    ").to(args.device)\n",
    "\n",
    "best_model_path = os.path.join(loading_dir, 'best_model.pth')\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {best_model_path}.\")\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=args.device))\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de31940-097a-4c97-87d6-6786ec3dc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE STS-B SCORE (SPEARMAN CORRELATION COEFFICIENT)\n",
    "# =============================================================================\n",
    "\n",
    "stsb_spearman = 100*evaluate_stsb(model, tokenizer, stsb_test_data, args)\n",
    "print(f\"STS-B Spearman Coefficient Score is:{stsb_spearman}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3711739-1bc0-4720-9e05-c70335c7ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE AND NORMALIZE EMBEDDINGS FOR TRAIN AND TEST SETS\n",
    "# =============================================================================\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- Generate Test Embeddings (STSb) ---\n",
    "print(\"Generating embeddings for the TEST dataset (STSb)...\")\n",
    "sents1_test = [ex['sentence1'] for ex in stsb_test_data]\n",
    "sents2_test = [ex['sentence2'] for ex in stsb_test_data]\n",
    "unique_test_sentences = list(set(sents1_test + sents2_test))\n",
    "test_embeddings_tensor = generate_embeddings(unique_test_sentences, model, tokenizer, args)\n",
    "# Normalize embeddings for PCA, as it's sensitive to vector magnitude\n",
    "test_embed = F.normalize(test_embeddings_tensor, p=2, dim=1).numpy()\n",
    "print(f\"Generated {test_embed.shape[0]} unique test embeddings of dimension {test_embed.shape[1]}.\")\n",
    "\n",
    "# --- Generate Train Embeddings (SNLI) ---\n",
    "print(\"Generating embeddings for the TRAIN dataset (SNLI)...\")\n",
    "train_sentences = list(set([s for p, h in train_dataset.samples for s in (p, h)]))\n",
    "train_embeddings_tensor = generate_embeddings(train_sentences, model, tokenizer, args)\n",
    "print(train_embeddings_tensor.shape)\n",
    "\n",
    "# --- Preparing a fixed random subset of Train Embeddings (SNLI) for feature correlation matrix analysis ---\n",
    "print(\"Preparing fixed random subset of Train Embeddings (SNLI) for feature correlation matrix analysis...\") \n",
    "num_train = len(train_embeddings_tensor[:,1])\n",
    "indices = list(range(num_train))\n",
    "split = int(0.9 * num_train)\n",
    "print(split)\n",
    "random.seed(args.seed)\n",
    "random.shuffle(indices)\n",
    "subset_indices = indices[split:]\n",
    "\n",
    "train_subset_embeddings_tensor = train_embeddings_tensor[subset_indices,:]\n",
    "\n",
    "# Normalize embeddings\n",
    "train_embed_norm = F.normalize(train_embeddings_tensor, p=2, dim=1).numpy()\n",
    "train_subset_embed_norm = F.normalize(train_subset_embeddings_tensor, p=2, dim=1).numpy()\n",
    "print(f\"Generated {train_embed_norm.shape[0]} unique train embeddings of dimension {train_embed_norm.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d7235-db1c-4a8e-abd2-8bca60c619d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE EFFECTIVE DIMENSIONALITY USING PCA ON TRAIN EMBEDDINGS\n",
    "# =============================================================================\n",
    "# The effective dimensionality is defined as the number of PCA components required to explain 99% variance of the train dataset\n",
    "\n",
    "# --- Fit a full PCA on the training data to find the optimal number of components ---\n",
    "print(\"\\n--- Fitting PCA on training data to analyze variance ---\")\n",
    "pca_full = PCA(n_components=None, random_state=42)\n",
    "pca_full.fit(train_embed_norm)\n",
    "\n",
    "# Find the number of components that explain 99% of the variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_for_99_variance = np.searchsorted(cumulative_variance, 0.99) + 1\n",
    "print(f\"Number of components to explain 99% of TRAIN variance: {n_for_99_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f82ee-7150-4592-a940-424d7304e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# QUANTIFY ROBUSTNESS USING PCA-BASED RMS RECONSTRUCTION ERROR ON TEST EMBEDDINGS\n",
    "# ===============================================================================\n",
    "\n",
    "print(f\"\\n--- Calculating Test Set Reconstruction Error using top {n_for_99_variance} components from train subspace ---\")\n",
    "\n",
    "# 1. Define a new PCA model with number of components equal to effective dimensionality defined in the previous cell\n",
    "pca_reconstruction = PCA(n_components=n_for_99_variance, random_state=42)\n",
    "\n",
    "# 2. Fit this PCA model ONLY on the training data to define the subspace\n",
    "pca_reconstruction.fit(train_embed_norm)\n",
    "\n",
    "# 3. Reconstruct the test data by projecting it onto the train subspace and then inverting\n",
    "test_projected = pca_reconstruction.transform(test_embed)\n",
    "test_reconstructed = pca_reconstruction.inverse_transform(test_projected)\n",
    "\n",
    "# 4. Calculate the Root Mean Squared Error between original and reconstructed test data\n",
    "reconstruction_rmse = np.sqrt(np.mean(np.square(test_embed - test_reconstructed)))\n",
    "print(f\"Test Set Reconstruction RMSE from Train Subspace: {reconstruction_rmse:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946b70e-87b3-4944-97e2-3c92c4588140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE FEATURE CORRELATION MATRIX, AND VISUALIZE AND STORE ITS CLUSTERMAP\n",
    "# =============================================================================\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"\"\" Generating feature covariance matrix computed using normalized output features for a fixed random subset of training data\"\"\")\n",
    "\n",
    "feat_corr_matrix = np.corrcoef(train_subset_embed_norm, rowvar=False)\n",
    "\n",
    "# Generate Correlation Clustermap\n",
    "\n",
    "print(f\"\"\" Generating clustermap of feature covariance matrix\"\"\")\n",
    "\n",
    "path_corr = os.path.join(loading_dir, f\"best_model_norm_output_feature_covariance_matrix.png\")\n",
    "\n",
    "g = sns.clustermap(\n",
    "    feat_corr_matrix,\n",
    "    cmap='RdBu_r',  # Red-Blue diverging colormap\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    figsize=(10, 10),\n",
    "    cbar_pos=(0.02, 0.8, 0.03, 0.15) # Position colorbar\n",
    ")\n",
    "\n",
    "g.ax_heatmap.set_xlabel('')\n",
    "g.ax_heatmap.set_ylabel('')\n",
    "g.fig.suptitle('MonoCon', fontsize=50, y=1.06)\n",
    "g.savefig(path_corr)\n",
    "\n",
    "print(f\"Plot saved to {path_corr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a3d99-be3a-4169-85df-b14a7d505660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# COMPUTE ROOT MEAN SQUARE OFF-DIAGONAL CORRELATION IN THE FEATURE CORRELATION MATRIX\n",
    "# ===================================================================================\n",
    "embed_dim = feat_corr_matrix.shape[0]\n",
    "mask = 1.0 - np.eye(embed_dim)\n",
    "rms_off_diagonal_feat_corr = np.sqrt(np.sum(np.square(feat_corr_matrix*mask))/(embed_dim*(embed_dim-1)))\n",
    "print(f\"RMS Off diagonal correlation in the feature correlation matrix is: {rms_off_diagonal_feat_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e8d56-4e88-4fa3-9ce9-a924c095c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# SAVE PERFORMANCE METRICS TO A JSON FILE\n",
    "# ===================================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "performance_metrics = {\n",
    "    \"STSb_Spearman\": float(stsb_spearman),\n",
    "    \"number of PCA components for 99% variance\": int(n_for_99_variance),\n",
    "    \"PCA-based RMS reconstruction error\": float(reconstruction_rmse),\n",
    "    \"RMS off-diagonal feature correlation\": float(rms_off_diagonal_feat_corr)\n",
    "}\n",
    "\n",
    "json_file_path = os.path.join(loading_dir, 'best_model_performance_metrics.json')\n",
    "with open(json_file_path, \"w\") as f:\n",
    "    json.dump(performance_metrics, f, indent=4)\n",
    "\n",
    "print(f\"Performance metrics saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b7113-f4aa-4b70-b92a-0169a880746e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
